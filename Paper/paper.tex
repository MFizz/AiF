%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.0 (13/4/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left

\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise

\newcommand{\ChapterCite}[1]{\textsc{Kapitel~\ref{#1}}}

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Title},pdfauthor={Author}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{Journal, Vol. XXI, No. 1, 1-5, 2013} % Journal information
\Archive{Additional note} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{Coalition skill games with incomplete information and over-abundance of tasks} % Article title

\Authors{Mihail Bogojeski, Tobias Pockrandt, Mitja Phillip Richter, Björn Fischer} % Authors


\Keywords{coalition games --- skills --- incomplete information --- matching problem} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{In diesem Paper stellen wir einen Lösungsansatz für das Matchen von Agenten mit gewissen Fertigkeiten auf Aufgaben welche besagte Fertigkeiten benötigen. Das Ziel ist es bei fehlen von vollständiger Information eine Maximierung der sozialen Wohlfahrt zu erreichen. Hierzu werden verschiedene Methoden und Konzepte aus Bereichen wie der Spieltheorie angewandt. Der vorgestellte Lösungsansatz wird ca. 80\% der theoretisch möglichen optimalen Lösung erreichen und wir liefern eine Reihe von Maßnahmen die eine Verbesserung dieses Wertes ermöglichen sollten, bzw. die Rechenzeit bei minimalem Verlust verbessern.}

%----------------------------------------------------------------------------------------

\begin{document}

\flushbottom % Makes all text pages the same height

\maketitle % Print the title and abstract box

\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction} % The \section*{} command stops section numbering
\label{sec:introduction}

\addcontentsline{toc}{section}{Introduction} % Adds this section to the table of contents

Dieses Paper stellt einen Abschlussbericht für den Kurs \textit{Agententechnologien in der Forschung} dar und repräsentiert die im Semester erarbeiten Resultate. Das matchen von Agenten und Aufgaben ist ein äußerst komplexes Problem. Aus diesem Grund werden einige Einschränkungen vorgenommen. Es existiert ein Pool aus Fertigkeiten und ihren Intensitäten. Aus diesem Pool werden die Aufgaben und die Fertigkeiten der Agenten entnommen. Zwischen der Menge von Agenten und der, der Aufgaben soll ein möglichst effizientes Matching gefunden werden. Effizienz definiert hierbei der Grad in dem die soziale Wohlfahrt maximiert wurde. Um einen erhöhten Bezug zu Realität zu erreichen besitzen die Agenten nur ein beschränktes Wissen über ihre Umgebung. Dies führt zusätzlich dazu das die beste Lösung nicht durch einfaches propagieren aller Permutationen ermittelbar ist. \\
Für die beschriebene Spezifikation des Problems wird in diesem Paper eine Lösung dargestellt. Hierfür wird zunächst auf die genauen Rahmenbedingungen des Versuchs eingegangen (\ChapterCite{sec:Umgebung}). Im Anschluss erfolgt eine genauere Betrachtung des angewandten Algorithmus (\ChapterCite{sec:Algorithmus}) sowie der genutzten Methoden und Konzepte (\ChapterCite{sec:Methoden}). Um die Qualität der präsentierten Lösung zu bewerten wurde eine Evaluation durchgeführt (\ChapterCite{sec:Evaluation}). Den Abschluss bilden ein Ausblick über mögliche Verbesserungen (\ChapterCite{sec:Future}), sowie eine Zusammenfassung (\ChapterCite{sec:Conclusion}). 

%------------------------------------------------

\section{Versuchsumgebung und Beschreibung}
\label{sec:Umgebung}

Um eine spannendere Umgebung zu erschaffen, bezieht sich der gewählte Algorithmus nicht einfach auf Agenten und Aufgaben. Um das gesetzte Problem interessanter und intuitiver darzustellen handelt es sich hier um Abenteurer (Agenten) die in einer Kaschemme angeheuert werden um gewisse Abenteuer (Aufgaben) gegen die Zahlung einer bestimmten Menge Gold zu erfüllen. Im folgenden wird daher auch von Abenteuerern und Abenteuer gesprochen. Die Abenteurer besitzen einen von 3 möglichen Skills (Fertigkeiten). Der Pool aus Skills besteht aus Kämpfen, Verhandeln und Schleichen. Jeder Fertigkeit eines Agenten ist ein Power-Wert zugeordnet, die beschreibt wie gut er diese beherrscht. Die Abenteuer benötigen von jedem Skill einen gewissen Wert um erfüllt zu werden. Eine Abenteuer kann nicht abgeschlossen werden, solange nicht alle Anforderungen komplett erfüllt worden sind.\\
Aufgrund des Überangebots von Aufgaben existieren mehr Abenteuer als Abenteurer, bzw. benötigen die Abenteuer mehr Power um gelöst zu werden als die Agenten in ihrer Summe verfügen. Sowohl die Abenteurer als auch die Abenteuer werden in einem gewissen Rahmen zufällig generiert. Für erstere wird ca. die Hälfte der Gesamtpower aller Abenteuer auf die Abenteurer verteilt. Die Abenteuer hingegen sind frei von derartigen Abhängigkeiten. Sie unterscheiden sich allerdings in ihre Größe (klein, mittel, groß), wobei viele kleine und wenig große Abenteuer existieren. Der Erlös einer Abenteuer im Vergleich zu ihrer Größe ist allerdings nicht linear, sodass das ein großes Abenteuer proportional mehr Gold ergibt als die Summe kleinerer mit gleicher Power. Dies entspricht auch der Realitätsnähe des Versuchs, da Großprojekte i.d.r. einen größeren Gewinn ausstoßen als kleinere.\\
Wie der Titel der Arbeit verrät wird hier mit unvollständigen Informationen gearbeitet. In diesem Fall bedeutet dies speziell, dass zwar jeder Abenteurer alle Abenteuer kennt, inklusive deren Bedarf und Erlös, allerdings besitzt der Agent keine konkreten Informationen darüber, wie viele andere Agenten im Spiel sind und insbesondere nicht wie viel Power sie besitzen.\\ 
In der Planungsphase des Versuchsaufbaus gab es die Diskussion ob die Abenteurer mit Solo- oder Multiskilled sein sollten. Für den Besitz von 2-3 Skills sprachen eine Interessantere Verhandlungsphase zwischen den Abenteurern. Dagegen standen der stark erhöhte Aufwand, sowie der Realismus, da eine Firma zu Meist auf eine Aufgabe Spezialisiert ist. Weiterhin bestand die unbewiesene Annahme, dass sich ein Multiskillagent auch als mehrere Soloskillagenten darstellen lässt. So führte auch nicht zuletzt die Tatsache, dass hier lediglich ein relativ ausgereifter Lösungsansatz vorgestellt werden soll, zu der Entscheidung Solokillagenten zu verwenden. \\
Weiterhin besitzen die Agenten für jedes Abenteuer gewisse Grundkosten die anfallen und mit dem Gewinn abgedeckt werden müssen. In unserem Beispiel ist das die Beschaffung von Tränken und die Instandhaltung von Waffen und Ausrüstung. In der Realität könnte sich dies durch Fahrtkosten oder Wartung von Maschinen darstellen lassen. Ein weiteres Ziel welches damit verfolgt wird, ist eine Erhöhung der Individualität der Einzelnen Agenten. Die Abenteurer unterliegen alle dem gleichen Verhalten, sodass sich ihre Handlungen nur durch ihre eigene Power und die Grundkosten unterscheidet. Ergo gibt es hier keine risikoscheuen oder risikoaverse Agenten. Für alle Agenten existiert ein Zeitlimit (Deadline) welches Bekannt ist, nachdem keine Bewerbungen auf Abenteuer mehr möglich sind, sodass die Abenteurer bei fortgeschrittener Zeit auch in weniger profitablere Alternativen wechseln um noch Gewinne zu machen.\\
Die Stellschrauben an denen hier initial gedreht werden kann, sind die Anzahl an Abenteurern und die der Abenteuer. Die Einhaltung der Rahmenbedingungen erfolgen anschließend automatisch. Eine genaue Beschreibung des Ablaufes des Algorithmus erfolgt im nächsten Kapitel.\\


%------------------------------------------------

\section{Algorithmusbeschreibung}
\label{sec:Algorithmus}
Zunächst erfolgt eine grobe Zusammenfassung des Algorithmus, wobei die einzelnen Bestandteile in den folgenden Unterkapiteln ausführlicher beleuchtet werden. Zu Beginn berechnen die Agenten für jedes Abenteuer ihren Nutzen. Anschließend dürfen sie sich auf bis zu 4. Abenteuer bewerben. Anhand dieser Bewerbungen werden Koalitionen gebildet und die Beste ausgewählt. Potentieller Überschuss in den Koalitionen wird entfernt und möglichst sinnvoll an die Agenten zurückgezahlt, da nur so ein erreichen des maximal möglichen Erlöses erst möglich wird. Der zu erwartende Gewinn wird virtuell an die Agenten verteilt, welche sich mit diesen Informationen auf Abenteuer bzw. Koalitionen festlegen. Letztlich werden die geschlossenen Abenteuer aus dem Spiel entfernt, zusammen mit der eingesetzten Power, die Auszahlungen finden statt und sofern noch Power im Spiel ist und die Zeit noch nicht abgelaufen ist beginnt der Algorithmus von neuem.

\paragraph{Bewerbungsphase:}
Jeder Agent berechnet seinen Nutzen für jedes Abenteuer und bewirbt sich für maximal vier Abenteuer, die den höchsten Nutzen für ihn haben. Die Agenten bewerben sich mit ihren ganzen verfügbaren Power, solange diese Power der benötigten Power des Abenteuers nicht überschreitet (in diesem Fall bewirbt sich der Agent mit der benötigten Power für das Skill exakt).  Die Bewerbungen aller Agenten werden aufgesammelt und anhand dieser werden die Koalitionen für jedes Abenteuer ermittelt. 

\paragraph{Koalitionsermittlung:}
Für jedes Abenteuer, werder zuerst alle mögliche Koalitionen aus der Agenten die sich für dieses Abenteuer beworben haben gebildet. Danach werden alle Koalitionen, die keine Gewinnerkoalitionen sind (d.h.\ die Koalitionen, die die Anforderungen des Abenteuers nicht erfüllen), eliminiert. Falls es keine Gewinnerkoalition gibt, wird die große Koalition als die einzige Koalition für dieses Abenteuer gespeichert. Schließlich, wird aus der Gewinnerkoalitionen der Banzhap Power für jedes Agent in diesem Abenteuer berechnet. In dieser Phase werden auch die Veto-Spieler in jedes Abenteuer ermittelt und ihren Banzhaf Power wird mit einem bestimmten Faktor multipliziert (in unser Implementierung verdoppelt), um die Macht der Veto-Spieler im Vergleich zu den anderen Spieler noch mehr zu vergrößern. 

\paragraph{Beste Koalition:}
Die beste Koalition für eines Abenteuer wird mit einer leicht geänderter Version des verteilten Algorithmus ermittelt. Zuerst werden die Koalitionen eliminiert, die einen nicht minimalen Überschuss an investierter Power haben. Unter Überschuss wird hier die Differenz zwischen der investierten Power der Koalition und der benötigten Power des Abenteuers für jedes Skill gemeint. Eine Koalition die einen höhen Überschuss hat ist daher nicht effizient, da die überflüssige Power potentiell bei anderen Abenteuer ausgenutzt werden könnte. So werden auch Dummy-Spieler aus der potentiell besten Koalition entfernt, da es immer eine Koalition ohne Dummy-Spielern und mit kleinerem Überschuss gibt. Der verteilte Algorithmus wird dann auf die Koalitionen mit minimalem Überschuss angewandt, wo die beste Koalition auf eine elitäre Weise ausgewählt wird. Genauer gesagt, die beste Koalition ist die Koalition, für die irgendeinen Agent die größte Gewinnschätzung hat.

\paragraph{Überschusstilgung:} 
In dieser Phase wird die überflüssige Power aus der besten Koalition entfernt und an den Agenten zurückgegeben. Da Power nur ganzzahlig werden kann, muss der Überschuss nur an bestimmten Agenten aufgeteilt werden. In unsere Implementierung wird die Aufteilung des Überschusses durch eine iterierte randomisierte Prozedur realisiert. Für jedes Skill bei dem es einen Überschuss gibt, wird in jede Iteration eine Power-Einheit einem Agenten zurückgegeben. Jeder Agent, der Power von einem Skilltyp investiert hat, kann mit einer gewissen Wahrscheinlichkeit eine Power-Einheit von der Überschuss in diese Skilltyp zurückbekommen. Diese Wahrscheinlichkeit ist gleich der Anteil der inverstierten Power des Agenten aus der gesamten inverstierten Power aller Agenten. Das heißt, dass Agenten die mehr Power aus einen Skilltyp inverstiert haben, mit einer höheren Wahrscheinlichkeit eine Power-Einheit aus der Überschuss zurückbekommen. Diese Prozedur wird so oft wiederholt, bis den ganzen Überschuss aus der besten Koalition entfernt wird.
)
\paragraph{Gewinnverteilung:}
Der Gewinn wird abhängig von der Banzhaf Power der Agenten aufgeteilt. Jeder Agent in der beste Koalition bekommt der gleichen Anteil der Gesamtgewinn, wie der Anteil seinen Banzhaf Power aus der Summe der Banzhaf Power aller Agenten. Dank der Banzhaf Power ist diese Verteilung fair bezüglich der Macht der Agenten und zusätzlich auch pareto-effizient, da der gesamte Gewinn verteilt wird.

\paragraph{Festlegung auf Abenteuer/Koaltion:}
Da sich die Agenten zu maximal vier Abenteuer mit voller Power bewerben können, muss sich jeder Agent auf nur ein Abenteuer festlegen. Die Entscheidung erfolgt in 2 Phasen: In der ersten Phase, wählt jeder Agent das Abenteuer aus, wo er am meisten gewinnen kann, d.h.\ wo seine Gewinnschätzung am höchsten ist. Für die zweite Phase werden diese Entscheidungen bekannt gegeben und die Agenten haben jetzt eine Möglichkeit ihre Entscheidung zu ändern. Diesmal entscheiden die Agenten nicht anhand ihrer Gewinnschätzung, sondern anhand ihrer Nutzenfunktion, die die Entscheidungen der anderen Agenten aus der ersten Phase miteinbezieht. Die Agenten wählen das Abenteuer mit dem größten Nutzen und diese Entscheidung ist für die aktuelle Runde endgültig.

\paragraph{Schliessung erfüllter Abenteuer:}
Nachdem sich jeder Agent für eine einzige Abenteuer entschieden hat, werden die Abenteuer, wo die Gewinnerkoalition vollständig ist (d.h.\ jeder Agent in der Koalition hat sich auf dieses Abenteuer festgelegt). Falls einen Abenteuer geschlossen wird, wird das Gewinn an der Agenten ausgezahlt, die investierte Power der Agenten aus ihren Power abgezogen und das geschlossene Abenteuer entfernt. Ab der nächste Runde kann sich kein Agent mehr für das geschlossene Abenteuer anmelden.

\section{Methoden und Konzepte}
\label{sec:Methoden}
Hier erfolgt eine Zusammenfassung der Methoden und Konzepte, welche unter anderem aus der Spieltheorie kommen um einen Überblick zu erhalten für potentielle Veränderungen aus \ChapterCite{sec:Future} oder Diskussionen für und gegen die angewandten bzw. nicht angewandten Mittel.

\paragraph{Gewinnschätzung:}
Die Gewinnschätzung des Agenten für einen Abenteuer ist eine ganz wichtige Funktion, die in viele Teile der Implementierung benutztz wird.
Für ein bestimmtes Abenteuer, wird die Gewinnschätzung wie folgt berechnet: Falls der Agent kein Mitglied der aktuellen Gewinnerkoalition des Abenteuers ist, schätzt er, dass er seinen fairen Anteil des Gewinns bekommt, d.h.\ er erwartet den gleichen Anteil des Gewinns, wie der Anteil seine investierte Power von der gesamten benötigten Power des Abenteuers. Falls der Agent Teil der großen Koalition ist, die aber nicht alle Anforderungen des Abenteuers erfüllen kann, schätzt der Agent das er der gleichen Anteil des Gewinns bekommt, wie der Anteil seine investierte Power von der gesamten Power der Koalition. Wenn der Agent ein Mitglied der aktuellen Gewinnerkolaition ist, kennt er seinen Banzhaf Power für dieses Abenteuer und schätzt sein Gewinn als der gleichen Anteil des gesamten Gewinns, wie der Anteil seiner Banzhaf Power aus der Summe der Banzhaf Power aller Agenten in dieser Koalition.

\paragraph{Nutzenfunktion:}
Die Nutzenfunktion ist das wichtigste Mittel, um das verhalten der Agenten zu steuern. Die Basis des Nutzen eines Agenten von einem bestimmten Abenteuer wird aus der Gewinnschätzung minus die Kosten für das Abenteuer gebildet. Dann wird dieser Wert abhängig von den Eigenschaften des Abenteuers angepasst und daraus der endgültige Nutzen erhalten. Es werden vier Eigenschaften eines Abenteuers und die zugehörige beste Koalition unterschieden:
\begin{itemize}
  \item[E1] Die Anzahl der bisherigen Fehlversuche bei diesem Abenteuer. Eine frühere Bewerbung zählt als Fehlversuch, wenn der Agent mit dieser Bewerbung kein Mitglied der Gewinnerkoalition geworden ist.
  \item[E2] Diese Eigenschaft spielt nur dann eine Rolle, wenn die große Koalition nicht alle Anforderungen des Abenteuers erfüllt. Es wird der Anteil der fehlenden Power der große Koalition zur Schließung des Abenteuers aus der gesamten benötigten Power für das Abenteuer berechnet. 
  \item[E3] Diese Eigenschaft spielt auch nur dann eine Rolle, wenn die große Koalition nicht alle Anforderungen des Abenteuers erfüllt. Es wird die Anzahl an Skills gezählt, bei denen die große Koalitionen einen Mangel an Power hat. Dieser Wert wird dann durch den Anzahl an insgesamt benötigten Skills für das Abenteuer dividiert.
  \item[E4] Diese Eigenschaft spielt in der zweite Phase des Festlegungsprotokolls eine Rolle. Sie wird genau wie E2 berechnet, aber statt der großen Koalition, wird hier die Anteil der fehlenden Power der Koalition aus der bereits festgelegten Agenten berechnet. Diese Eigenschaft drückt also aus, wie viel Power benötigen die bereits festgelegten Agenten um das Abenteuer abzuschließen. 
\end{itemize}

Wie bei der Gewinnschätzung, wird in der Nutzenfunktion zwischen drei Fälle unterschieden, wobei in jedem Fall unterschiedliche Eigenschaften des Abenteuers einen Einfluss auf der ursprünglichen Gewinnschätzung haben. Im ersten Fall ist der Agent kein Mitglied der Gewinnerkoalition des Abenteuers und die Gewinnschätzung wird von Eigenschaft E1 und der Anzahl an vergangenen Runden negativ beeinflusst, d.h.\ der Nutzen für so einen Abenteuer wird später im Spiel und mit steigendem Anzahl an Fehlversuche immer kleiner. \\
Der zweite Fall tritt auf, wenn der Agent Teil der großen Koalition ist, die aber nicht alle Anforderungen des Abenteuers erfüllen kann. Die Gewinnschätzung wird in diesem Fall von E1 und der Anzahl an vergangenen Runden auch negativ beeinflusst, kann sich aber in Abhängigkeit von E2 und E3 vergrößern oder verkleinern. Die Nutzen ist daher in diesem Fall größer, falls die große Koalition nur wenig Power benötigt um das Abenteuer abzuschließen. \\
Im dritten Fall, wenn der Agent einen Mitglied der Gewinnerkoalition ist, wird die Gewinnschätzung von der Anzahl an vergangenen Runden stark positiv beeinflusst, d.h.\ der Nutzen eines Abenteuers im diesen Fall steigt in den letzten Runden sehr stark. Außerdem wird in den letzten Runden der positive Einfluss von Eigenschaft E4 auch viel stärker. Also die Nutzen in dritten Fall wird größer, wenn die bereits festgelegten Agenten wenig Power benötigen, um das Abenteuer zu schliessen, und diesen Nuzten steigt in den letzten Runden besonders viel.\\
So einen Nutzenfunktion stellt sicher, dass die Agenten am Anfang mit ihren Bewerbungen flexibel sind und versuchen, einen maximalen Gewinn zu erzielen. Andererseits, in den späteren Runden, versucht jeder Agent vor dem Deadline irgendein Gewinn zu erzielen, auch wenn dieser für ihn nicht optimal ist. In diese weise wird ein realistisches Verhalten der Agenten modelliert. 


\paragraph{Banzhaf-Power:}
Um die Banzhaf-Power eines Agenten zu bestimmen wird gezählt in wie vielen möglichen Koalitionen dieser ein kritischer Spieler ist. Wie vorher erwähnt, wird die Banzhaf-Power der Veto-Spieler zusätzlich mit einem Faktor multipliziert, um die Tatsache auszudrücken, dass Veto-Spieler viel mächtiger als den anderen Spielern sind.

\paragraph{Egalitäre und elitäre Funktionen:} Für die Ermittlung der besten Koalition haben wir auch eine egalitäre Version des Algorithmus getestet, wo wir die Koalition, mit dem größten minimalen Nutzen für einen Agent, als die beste Koalition ausgewählt haben. Diese Version des Algorithmus hat schlechtere, aber nicht signifikant unterschiedliche Ergebnisse als die elitäre Version erzielt.


\paragraph{Deadline:}
Die Deadline war ein ganz wichtiges Konzept in unser Implementierung, das das Verhalten der Agenten stark beeinflusst hat. Dank der Deadline ist der Spielzustand in jeder Runde ein bisschen anders, was ein zyklisches Verhalten der Agenten verhindert. Die Deadline hat auch einen großen Einfluss auf der Konvergenz unser Algorithmus, weil das Verhalten der Agenten in den letzen Runden vor der Deadline viel stabiler wird. So wird sicher gestellt, dass die Agenten in den letzten Runden so viele Abenteuer wie möglich abschließen, obwohl diese Abenteuer für jeden einzelnen Agent nicht einen optimalen Gewinn liefern.

\section{Evaluation}
\label{sec:Evaluation}
\paragraph{Testaufbau und Initialisierung:}
Die Evaluation unser Implementieren war mit verschiedenen Kombinationen von 10-15 Agenten, 10-20 Abenteuer und 10-50 Runden durchgeführt. Abenteuer mit Anforderungen von verschiedenen Größen werden von einer Beta-Verteilung mit Parametern $\alpha=2.3$ und $\beta=4.6$ generiert. Für jedes Abenteuer wird das so generierte Power an zwei oder drie Skills baliebig verteilt. Schließlich, wird der Gewinn eines Abenteuers mit eine superlineare Funktion aus der benötigten Power zusammen mit einem gewissen randomisierten Rauschen ausgerechnet. So ist eine Power-Einheit, die in einem Abenteuer mit höheren Anfordeungen inverstiert ist, im Schnitt mehr wert als eine Power-Einheit in einem Abenteuer mit nidrigen Anforderungen.\\
Solbald alle Abenteuer generiert sind, wird der angegebenen Anzahl der Agenten generiert, sodass für jedes Skill, die gesamte Power aller Agenten 40-50\% der gesamten benotigten Power aller Abenteuer beträgt. So wird sichergestellt, dass es eine Überangebot an Abenteuer gibt. Die Anzahl der Agenten, die einen bestimmten Skill besitzen, wird in Abhängigkeit von der Höhe der benötigten Power für dieses Skill in allen Abenteuern bestimmt. Das heißt, wenn für einen bestimmten Skill in allen Abenteuern mehr Power als den anderen Skills benötigt wird, werden auch mehr Agenten diesen Skill besitzen. So werden alle Agenten im schnitt den gleichen Power haben, unabhängig von welchen Skill sie besitzen. Als letztes werden die Kosten der Agenten für jedes Abeuteuer in einem bestimmten Intevall beliebig generiert.

\paragraph{Greedy Bound und Upper Bound:}
Die optimale Lösung, und damit der maximal erreichbaren Gewinn für einen Spiel auszurechnen ist sehr auswändig (equivalent zu einer Lösung der multidimensionalen Rücksackproblem, was NP-schwer ist). Deswegen haben wir haben wir zwei heuristische Werte für jedes Spiel ausgerechnet: die Greedy und Upper Bound, die wir mit dem von unser Implementierung erzielten Gewinn vergleichen, um unser Algorithmus zu evaluieren. \\
Die Greedy Bound ist der Gewinn, der mit einem Greedy Algorithmus mit vollstandiger Information im Spiel erzielen kann. Der Greedy Algorithmus schliesst in jede Iteration das Abenteuer mit dem höchsten Gewinn das geschlossen werden kann, bis es kein Abenteuer gibt das geschlossen werden kann. Der Greedy Algorithmus liefert natürlich nicht immer die optimale Lösung, ist aber in der Regel relativ nah dem Optimum.\\
Die Upper Bound ist dann eine echte Obergrenze, d.h.\ die Gewinn von der Upper Bound ist immer gleich oder höher als dem maximal erreichbaren Gewinn. Dieser Gewinn wird wie folgt berechnet: In jeder Iteration wird versucht, das Abenteuer mit dem höchsten Gewinn zu schließen. Falls das möglich ist, wird das Abenteuer geschlossen und der Algorithmus geht weiter. Wenn die Agenten nicht genug Power haben dieses Abenteuer zu schliessen, wird so viel Power wie möglich in das Abenteuer investiert, und der gleichen Anteil des Gewinns an den Agenten ausgezahlt, wie der Anteil der investierten Power von der benötigten Power des Abenteuers. Dieser Schritt wird dann so lange wiederholt, bis die Agenten kein Power mehr zu Investieren haben. Die so ausgerechnete Upper Bound, ist deswegen höher als dem maximal erreichbaren Gewinn, weil der Gewinn eines Abenteuers superlinear von der benötigten Power abhängig ist.

\section{Future Work}
\label{sec:Future}

Wie bereits aus der Evaluation des vorherigen Kapitels, bekannt erreicht der von uns entwickelte Algorithmus eine recht akzeptable Leistung, wobei durchaus noch Verbesserungen bzw. Veränderungen möglich und notwendig sind. Diese weiteren Arbeiten können und sollen unter anderem die Effizienz (nähe zum Optimum) als auch die Geschwindigkeit verbessern. Es sind unter anderem auf Maßnahmen aufgelistete, welche den Informationsgehalt des Versuchs erhöhen, indem z.B. mehr (spezifischere) Daten betrachtet und damit auch berücksichtigt und ausgelesen werden können.

\paragraph{Verteilung des Algorithmus:}
Die hier gewählte Implementation ist kein echter verteilter Algorithmus. Die Hauptarbeit wird von einer zentralen Instanz durchgeführt, welche die Berechnungen und Entscheidungen für die Agenten (in deren Sinne) durchführt. Wenn jeder Agent als Einzelne Entität in einem verteilten Algorithmus realisiert ist, lassen sich sehr einfach individuelle Verhaltensmuster für jeweilige Abenteuerer festlegen. Dann können unter anderem verschiedene Risikoaspekte mit in den Aufbau eingewoben werden. Was bedeutet risikoscheue und averse Agenten können aufeinander Treffen. \\
Ein weiterer Aspekt könnte sein, das ein Agent A unter keinen Umständen mit einem Agenten B	eine Koalition bilden möchte, aufgrund schlechter Erfahrungen oder ähnliches. Hierbei könnte untersucht werden in wie weit die Effizienz des gesamten Konstruktes, durch den Ausschluss der Zusammenarbeit zweier Agenten, in Mitleidenschaft gezogen wird. Der Kreativität im Design vom Verhalten einzelner Agenten ist hierbei kein Limit gesetzt.

\paragraph{Optimierung der Koalitionsbildung:}
Da aktuell jeder Koalition berechnet werden muss, aufgrund des Banzhafpowerindex befindet sich der Aufwand in einem Exponentiellen Bereich, was dazu führt, dass der Algorithmus schon bei einer vergleichsweise geringen Anzahl an Abenteurern extrem viel Zeit benötigt. Es muss demnach eine Möglichkeit gefunden werden ein im besten Fall identisches Ergebnis in geringerer Zeit zu finden. Der intuitive Ansatz ist hierbei nicht alle Koalitionen zu betrachten, sondern eine simple Vorauswahl zu treffen welche die Anzahl einschränkt. Wie diese Auszusehen hat und wie groß der Effizienzverlust ist muss ermittelt und evaluiert werden.

\paragraph{Maschinelles Lernen:} 
In der von uns entwickelten Lösung ist Maschinelles Lernen nur sehr marginal vertreten. Agenten merken sich unter anderem wie oft sie bereits an einer Koalition gescheitert sind. Wenn dieser Anteil erheblich ausgebaut werden würde und sich jede Aktion und ihr Resultat quasi gespeichert wird, lassen sich unter Umständen ganz neue Lösungsverhalten finden, welche das Optimum näher erreichen. Hierbei sind allerdings nicht Lernphase über die Dauer eines Spiels gemeint, sondern lernen anhand von 1000 und oder mehr Spielen das anzustrebende Ergebnis. Sodass sich mittels Lernen unter Umständen neue oder unerwartete Lösungen mit ähnlichem bzw. besserem Ergebnis finden lassen. Dieser Lernende Agent kann gegen "gewöhnliche" Agenten oder sogar Menschen Spielen, diese schnell Anhand ihres Verhaltens identifizieren bzw. kategorisieren und eine geeignete Strategien anzuwenden. Dies sei nur als Anreiz für die Einbettung maschinellen Lernens zu verstehen und erhebt nicht den Anspruch vollständig zu sein.

\paragraph{Untersuchung aller Parameter:}
Das großflächige Untersuchen sämtlicher Parameter könnte aufschlussreiche Ergebnisse liefern. In dieser Evaluation wurde nur an der Menge von Agenten und Abenteuern manipuliert. Weitere Stellschrauben an denen zu drehen ist, sind:
\begin{itemize}
	\item \textit{Belohnung}: Die Verteilung von Auszahlungen könnte variiert werden. Den Anstieg noch mehr Quadratische, oder linear oder mittels einer Beliebigen Fkt darstellen. Auch ein stagnierender Anstieg der Auszahlung für größere Abenteuer ist denkbar.
	\item \textit{Skillmengen}: Statt 3 Skills, könnte eine Beliebige Anzahl an Skills vorhanden sein (z.B. 5). Eine weitere denkbare Alternative ist, das nicht jede Baustelle alle Skills braucht, oder die Agenten mehr als nur einen Skill besitzen, wodurch die Verhandlung eine weitere Dimension erreicht. 
	\item \textit{Powergrößen}: Die Power die jeder Agent hat bzw. alle Agenten haben oder die Abenteuer haben, kann weiterhin variiert werden.
	\item \textit{Angebot}: Zum einen kann das Überangebot der Aufgaben noch verschärft oder zu ein Unterangebot abgewandelt werden. Dann würde sich auch die Frage nach dem Ergebnis abwandeln, da bei dieser Veränderung nicht von Interesse ist, wie viel Gold alle Agenten Erwirtschaften, sondern wer bekommt grundsätzlich Gewinn und wie viel. \\
	Ein weiteres Szenario ist eine komplette Gleichverteilung. Es ist demnach möglich alle Abenteuer zu lösen ohne das ein Skillpunkt eines Abenteurers übrig bleibt. Hier stellt sich die Frage, welcher Agent wie viel vom Gewinn erwirtschaften kann.
	\item \textit{Agenten}: Die verschiedenen Verhalten die ein Agent aufweisen kann wurden bereits erläutert.
\end{itemize}




\section{Conclusion}
\label{sec:Conclusion}

%------------------------------------------------
\phantomsection
\section*{Acknowledgments} % The \section*{} command stops section numbering

\addcontentsline{toc}{section}{Acknowledgments} % Adds this section to the table of contents

So long and thanks for all the fish \cite{Figueredo:2009dg}.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\phantomsection
\bibliographystyle{unsrt}
\bibliography{sample}

%----------------------------------------------------------------------------------------

\end{document}
